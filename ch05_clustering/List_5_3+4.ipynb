{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List_5_3+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List_5_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from bregman.suite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5b12d3090827>:3: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donkee/mytensorflow/venv/lib/python2.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/donkee/mytensorflow/venv/lib/python2.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/donkee/mytensorflow/venv/lib/python2.7/site-packages/tensorflow/python/training/input.py:199: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/donkee/mytensorflow/venv/lib/python2.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-2-5b12d3090827>:4: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.map(tf.read_file)`.\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.train.match_filenames_once('./audio_dataset/*.wav')\n",
    "count_num_files = tf.size(filenames)\n",
    "filename_queue = tf.train.string_input_producer(filenames)\n",
    "reader = tf.WholeFileReader()\n",
    "filename, file_contents = reader.read(filename_queue)\n",
    "\n",
    "chroma = tf.placeholder(tf.float32)\n",
    "max_freqs = tf.argmax(chroma, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_chromagram(sess):\n",
    "    audio_file = sess.run(filename)\n",
    "    F = Chromagram(audio_file, nfft=16384, wfft=8192, nhop=2205)\n",
    "    return F.X\n",
    "\n",
    "def extract_feature_vector(sess, chroma_data):\n",
    "    num_features, num_samples = np.shape(chroma_data)\n",
    "    freq_vals = sess.run(max_freqs, feed_dict={chroma: chroma_data})\n",
    "    hist, bins = np.histogram(freq_vals, bins=range(num_features + 1))\n",
    "    return hist.astype(float) / num_samples\n",
    "\n",
    "def get_dataset(sess):\n",
    "    num_files = sess.run(count_num_files)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    xs = []\n",
    "    for _ in range(num_files):\n",
    "        chroma_data = get_next_chromagram(sess)\n",
    "        x = [extract_feature_vector(sess, chroma_data)]\n",
    "        x = np.matrix(x)\n",
    "        if len(xs) == 0:\n",
    "            xs = x\n",
    "        else:\n",
    "            xs = np.vstack((xs, x))\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List_5_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_cluster_centroids(X, k):\n",
    "    return X[0:k, :]\n",
    "\n",
    "def assign_cluster(X, centroids):\n",
    "    expanded_vectors = tf.expand_dims(X, 0)\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1)\n",
    "    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors,\n",
    "      expanded_centroids)), 2)\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    return mins\n",
    "\n",
    "def recompute_centroids(X, Y):\n",
    "    sums = tf.unsorted_segment_sum(X, Y, k)\n",
    "    counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)\n",
    "    return sums / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids\n",
      "[[0.20750733 0.07575758 0.08406647 0.0259042  0.04363636 0.06848485\n",
      "  0.16482893 0.10408602 0.05075269 0.06060606 0.08406647 0.03030303]\n",
      " [0.03533354 0.         0.         0.02040816 0.60234542 0.25982333\n",
      "  0.         0.08208955 0.         0.         0.         0.        ]]\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[{{node input_producer/input_producer_EnqueueMany}}]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    X = get_dataset(sess)\n",
    "    centroids = initial_cluster_centroids(X, k)\n",
    "    i, converged = 0, False\n",
    "    while not converged and i < max_iterations:\n",
    "        i += 1\n",
    "        Y = assign_cluster(X, centroids)\n",
    "        centroids = sess.run(recompute_centroids(X, Y))\n",
    "    print(\"centroids: \")\n",
    "    print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
